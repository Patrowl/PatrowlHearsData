{"cve": {"data_type": "CVE", "data_format": "MITRE", "data_version": "4.0", "CVE_data_meta": {"ID": "CVE-2023-29374", "ASSIGNER": "cve@mitre.org"}, "problemtype": {"problemtype_data": [{"description": []}]}, "references": {"reference_data": [{"url": "https://twitter.com/rharang/status/1641899743608463365/photo/1", "name": "https://twitter.com/rharang/status/1641899743608463365/photo/1", "refsource": "MISC", "tags": []}, {"url": "https://github.com/hwchase17/langchain/pull/1119", "name": "https://github.com/hwchase17/langchain/pull/1119", "refsource": "MISC", "tags": []}, {"url": "https://github.com/hwchase17/langchain/issues/814", "name": "https://github.com/hwchase17/langchain/issues/814", "refsource": "MISC", "tags": []}, {"url": "https://github.com/hwchase17/langchain/issues/1026", "name": "https://github.com/hwchase17/langchain/issues/1026", "refsource": "MISC", "tags": []}]}, "description": {"description_data": [{"lang": "en", "value": "In LangChain through 0.0.131, the LLMMathChain chain allows prompt injection attacks that can execute arbitrary code via the Python exec method."}]}}, "configurations": {"CVE_data_version": "4.0", "nodes": []}, "impact": {}, "publishedDate": "2023-04-05T02:15Z", "lastModifiedDate": "2023-04-05T13:02Z"}