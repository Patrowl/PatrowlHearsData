{"publishedDate": "2024-05-01T06:15Z", "lastModifiedDate": "2025-04-08T18:53Z", "cve": {"data_type": "CVE", "data_format": "MITRE", "data_version": "1.0", "CVE_data_meta": {"ID": "CVE-2024-26976", "ASSIGNER": "416baaa9-dc9f-4396-8d5f-8c081fb06d67"}, "description": {"description_data": [{"lang": "en", "value": "In the Linux kernel, the following vulnerability has been resolved:\n\nKVM: Always flush async #PF workqueue when vCPU is being destroyed\n\nAlways flush the per-vCPU async #PF workqueue when a vCPU is clearing its\ncompletion queue, e.g. when a VM and all its vCPUs is being destroyed.\nKVM must ensure that none of its workqueue callbacks is running when the\nlast reference to the KVM _module_ is put.  Gifting a reference to the\nassociated VM prevents the workqueue callback from dereferencing freed\nvCPU/VM memory, but does not prevent the KVM module from being unloaded\nbefore the callback completes.\n\nDrop the misguided VM refcount gifting, as calling kvm_put_kvm() from\nasync_pf_execute() if kvm_put_kvm() flushes the async #PF workqueue will\nresult in deadlock.  async_pf_execute() can't return until kvm_put_kvm()\nfinishes, and kvm_put_kvm() can't return until async_pf_execute() finishes:\n\n WARNING: CPU: 8 PID: 251 at virt/kvm/kvm_main.c:1435 kvm_put_kvm+0x2d/0x320 [kvm]\n Modules linked in: vhost_net vhost vhost_iotlb tap kvm_intel kvm irqbypass\n CPU: 8 PID: 251 Comm: kworker/8:1 Tainted: G        W          6.6.0-rc1-e7af8d17224a-x86/gmem-vm #119\n Hardware name: QEMU Standard PC (Q35 + ICH9, 2009), BIOS 0.0.0 02/06/2015\n Workqueue: events async_pf_execute [kvm]\n RIP: 0010:kvm_put_kvm+0x2d/0x320 [kvm]\n Call Trace:\n  <TASK>\n  async_pf_execute+0x198/0x260 [kvm]\n  process_one_work+0x145/0x2d0\n  worker_thread+0x27e/0x3a0\n  kthread+0xba/0xe0\n  ret_from_fork+0x2d/0x50\n  ret_from_fork_asm+0x11/0x20\n  </TASK>\n ---[ end trace 0000000000000000 ]---\n INFO: task kworker/8:1:251 blocked for more than 120 seconds.\n       Tainted: G        W          6.6.0-rc1-e7af8d17224a-x86/gmem-vm #119\n \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\n task:kworker/8:1     state:D stack:0     pid:251   ppid:2      flags:0x00004000\n Workqueue: events async_pf_execute [kvm]\n Call Trace:\n  <TASK>\n  __schedule+0x33f/0xa40\n  schedule+0x53/0xc0\n  schedule_timeout+0x12a/0x140\n  __wait_for_common+0x8d/0x1d0\n  __flush_work.isra.0+0x19f/0x2c0\n  kvm_clear_async_pf_completion_queue+0x129/0x190 [kvm]\n  kvm_arch_destroy_vm+0x78/0x1b0 [kvm]\n  kvm_put_kvm+0x1c1/0x320 [kvm]\n  async_pf_execute+0x198/0x260 [kvm]\n  process_one_work+0x145/0x2d0\n  worker_thread+0x27e/0x3a0\n  kthread+0xba/0xe0\n  ret_from_fork+0x2d/0x50\n  ret_from_fork_asm+0x11/0x20\n  </TASK>\n\nIf kvm_clear_async_pf_completion_queue() actually flushes the workqueue,\nthen there's no need to gift async_pf_execute() a reference because all\ninvocations of async_pf_execute() will be forced to complete before the\nvCPU and its VM are destroyed/freed.  And that in turn fixes the module\nunloading bug as __fput() won't do module_put() on the last vCPU reference\nuntil the vCPU has been freed, e.g. if closing the vCPU file also puts the\nlast reference to the KVM module.\n\nNote that kvm_check_async_pf_completion() may also take the work item off\nthe completion queue and so also needs to flush the work queue, as the\nwork will not be seen by kvm_clear_async_pf_completion_queue().  Waiting\non the workqueue could theoretically delay a vCPU due to waiting for the\nwork to complete, but that's a very, very small chance, and likely a very\nsmall delay.  kvm_arch_async_page_present_queued() unconditionally makes a\nnew request, i.e. will effectively delay entering the guest, so the\nremaining work is really just:\n\n        trace_kvm_async_pf_completed(addr, cr2_or_gpa);\n\n        __kvm_vcpu_wake_up(vcpu);\n\n        mmput(mm);\n\nand mmput() can't drop the last reference to the page tables if the vCPU is\nstill alive, i.e. the vCPU won't get stuck tearing down page tables.\n\nAdd a helper to do the flushing, specifically to deal with \"wakeup all\"\nwork items, as they aren't actually work items, i.e. are never placed in a\nworkqueue.  Trying to flush a bogus workqueue entry rightly makes\n__flush_work() complain (kudos to whoever added that sanity check).\n\nNote, commit 5f6de5cbebee (\"KVM: Prevent module exit until al\n---truncated---"}, {"lang": "es", "value": "En el kernel de Linux, se ha resuelto la siguiente vulnerabilidad: KVM: siempre vac\u00ede la cola de trabajo as\u00edncrona #PF cuando se destruya la vCPU. Siempre vac\u00ede la cola de trabajo as\u00edncrona #PF por vCPU cuando una vCPU est\u00e9 limpiando su cola de finalizaci\u00f3n, por ejemplo, cuando una VM y todo sus vCPU est\u00e1n siendo destruidas. KVM debe asegurarse de que ninguna de sus devoluciones de llamada de la cola de trabajo se est\u00e9 ejecutando cuando se coloca la \u00faltima referencia al _m\u00f3dulo_ KVM. Regalar una referencia a la VM asociada evita que la devoluci\u00f3n de llamada de la cola de trabajo elimine la referencia a la memoria de vCPU/VM liberada, pero no evita que el m\u00f3dulo KVM se descargue antes de que se complete la devoluci\u00f3n de llamada. Elimine el regalo de recuento de VM equivocado, ya que llamar a kvm_put_kvm() desde async_pf_execute() si kvm_put_kvm() vac\u00eda la cola de trabajo as\u00edncrona #PF resultar\u00e1 en un punto muerto. async_pf_execute() no puede regresar hasta que finalice kvm_put_kvm(), y kvm_put_kvm() no puede regresar hasta que finalice async_pf_execute(): ADVERTENCIA: CPU: 8 PID: 251 en virt/kvm/kvm_main.c:1435 kvm_put_kvm+0x2d/0x320 [kvm] M\u00f3dulos vinculados en: vhost_net vhost vhost_iotlb tap kvm_intel kvm irqbypass CPU: 8 PID: 251 Comm: kworker/8:1 Tainted: GW 6.6.0-rc1-e7af8d17224a-x86/gmem-vm #119 Nombre de hardware: Est\u00e1ndar QEMU PC (Q35 + ICH9, 2009), BIOS 0.0.0 06/02/2015 Cola de trabajo: eventos async_pf_execute [kvm] RIP: 0010:kvm_put_kvm+0x2d/0x320 [kvm] Seguimiento de llamadas:  async_pf_execute+0x198/0x260 [kvm ] Process_one_work+0x145/0x2d0 work_thread+0x27e/0x3a0 kthread+0xba/0xe0 ret_from_fork+0x2d/0x50 ret_from_fork_asm+0x11/0x20  ---[ end trace 0000000000000000 ]--- INFORMACI\u00d3N: tarea kworker /8:1: 251 bloqueado durante m\u00e1s de 120 segundos. Contaminado: GW 6.6.0-rc1-e7af8d17224a-x86/gmem-vm #119 \"echo 0 &gt; /proc/sys/kernel/hung_task_timeout_secs\" desactiva este mensaje. tarea:kworker/8:1 estado:D pila:0 pid:251 ppid:2 banderas:0x00004000 Cola de trabajo: eventos async_pf_execute [kvm] Seguimiento de llamadas:  __schedule+0x33f/0xa40 Schedule+0x53/0xc0 Schedule_timeout+0x12a/0x140 __wait_for_common+0x8d/0x1d0 __flush_work.isra.0+0x19f/0x2c0 kvm_clear_async_pf_completion_queue+0x129/0x190 [kvm] kvm_arch_destroy_vm+0x78/0x1b0 [kvm] x320 [kvm] async_pf_execute+0x198/0x260 [kvm] proceso_one_work+0x145/ 0x2d0 trabajador_thread+0x27e/0x3a0 kthread+0xba/0xe0 ret_from_fork+0x2d/0x50 ret_from_fork_asm+0x11/0x20  Si kvm_clear_async_pf_completion_queue() realmente vac\u00eda la cola de trabajo, entonces no hay necesidad de regalar un referencia porque todas las invocaciones de async_pf_execute () se ver\u00e1 obligado a completarse antes de que la vCPU y su VM sean destruidas o liberadas. Y eso, a su vez, corrige el error de descarga del m\u00f3dulo, ya que __fput() no ejecutar\u00e1 module_put() en la \u00faltima referencia de vCPU hasta que se haya liberado la vCPU, por ejemplo, si al cerrar el archivo de vCPU tambi\u00e9n se coloca la \u00faltima referencia al m\u00f3dulo KVM. Tenga en cuenta que kvm_check_async_pf_completion() tambi\u00e9n puede sacar el elemento de trabajo de la cola de finalizaci\u00f3n y, por lo tanto, tambi\u00e9n necesita vaciar la cola de trabajos, ya que kvm_clear_async_pf_completion_queue() no ver\u00e1 el trabajo. En teor\u00eda, esperar en la cola de trabajo podr\u00eda retrasar una vCPU debido a la espera de que se complete el trabajo, pero esa es una posibilidad muy, muy peque\u00f1a y probablemente un retraso muy peque\u00f1o. kvm_arch_async_page_present_queued() realiza incondicionalmente una nueva solicitud, es decir, efectivamente retrasar\u00e1 la entrada del invitado, por lo que el trabajo restante es realmente solo: trace_kvm_async_pf_completed(addr, cr2_or_gpa); __kvm_vcpu_wake_up(vcpu); mmput(mm); y mmput() no puede eliminar la \u00faltima referencia a las tablas de p\u00e1ginas si la vCPU a\u00fan est\u00e1 activa, es decir, la vCPU no se atascar\u00e1 al derribar las tablas de p\u00e1ginas. ---truncado---"}]}, "references": {"reference_data": [{"url": "https://git.kernel.org/stable/c/3d75b8aa5c29058a512db29da7cbee8052724157", "name": "", "refsource": "", "tags": ["Patch"]}, {"url": "https://git.kernel.org/stable/c/4f3a3bce428fb439c66a578adc447afce7b4a750", "name": "", "refsource": "", "tags": ["Patch"]}, {"url": "https://git.kernel.org/stable/c/82e25cc1c2e93c3023da98be282322fc08b61ffb", "name": "", "refsource": "", "tags": ["Patch"]}, {"url": "https://git.kernel.org/stable/c/83d3c5e309611ef593e2fcb78444fc8ceedf9bac", "name": "", "refsource": "", "tags": ["Patch"]}, {"url": "https://git.kernel.org/stable/c/a75afe480d4349c524d9c659b1a5a544dbc39a98", "name": "", "refsource": "", "tags": ["Patch"]}, {"url": "https://git.kernel.org/stable/c/ab2c2f5d9576112ad22cfd3798071cb74693b1f5", "name": "", "refsource": "", "tags": ["Patch"]}, {"url": "https://git.kernel.org/stable/c/b54478d20375874aeee257744dedfd3e413432ff", "name": "", "refsource": "", "tags": ["Patch"]}, {"url": "https://git.kernel.org/stable/c/caa9af2e27c275e089d702cfbaaece3b42bca31b", "name": "", "refsource": "", "tags": ["Patch"]}, {"url": "https://git.kernel.org/stable/c/f8730d6335e5f43d09151fca1f0f41922209a264", "name": "", "refsource": "", "tags": ["Patch"]}, {"url": "https://git.kernel.org/stable/c/3d75b8aa5c29058a512db29da7cbee8052724157", "name": "", "refsource": "", "tags": ["Patch"]}, {"url": "https://git.kernel.org/stable/c/4f3a3bce428fb439c66a578adc447afce7b4a750", "name": "", "refsource": "", "tags": ["Patch"]}, {"url": "https://git.kernel.org/stable/c/82e25cc1c2e93c3023da98be282322fc08b61ffb", "name": "", "refsource": "", "tags": ["Patch"]}, {"url": "https://git.kernel.org/stable/c/83d3c5e309611ef593e2fcb78444fc8ceedf9bac", "name": "", "refsource": "", "tags": ["Patch"]}, {"url": "https://git.kernel.org/stable/c/a75afe480d4349c524d9c659b1a5a544dbc39a98", "name": "", "refsource": "", "tags": ["Patch"]}, {"url": "https://git.kernel.org/stable/c/ab2c2f5d9576112ad22cfd3798071cb74693b1f5", "name": "", "refsource": "", "tags": ["Patch"]}, {"url": "https://git.kernel.org/stable/c/b54478d20375874aeee257744dedfd3e413432ff", "name": "", "refsource": "", "tags": ["Patch"]}, {"url": "https://git.kernel.org/stable/c/caa9af2e27c275e089d702cfbaaece3b42bca31b", "name": "", "refsource": "", "tags": ["Patch"]}, {"url": "https://git.kernel.org/stable/c/f8730d6335e5f43d09151fca1f0f41922209a264", "name": "", "refsource": "", "tags": ["Patch"]}, {"url": "https://lists.debian.org/debian-lts-announce/2024/06/msg00017.html", "name": "", "refsource": "", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "https://lists.debian.org/debian-lts-announce/2024/06/msg00020.html", "name": "", "refsource": "", "tags": ["Mailing List", "Third Party Advisory"]}]}, "problemtype": {"problemtype_data": [{"description": [{"lang": "en", "value": "CWE-400"}]}]}}, "impact": {"baseMetricV3": {"exploitabilityScore": 1.0, "impactScore": 5.9, "cvssV3": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:H/PR:L/UI:N/S:U/C:H/I:H/A:H", "baseScore": 7.0, "baseSeverity": "HIGH", "attackVector": "LOCAL", "attackComplexity": "HIGH", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH"}}}, "configurations": {"CVE_data_version": "4.0", "nodes": [{"operator": "OR", "negate": false, "children": [], "cpe_match": [{"vulnerable": true, "cpe23Uri": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "matchCriteriaId": "1D7CB946-0FE3-48B8-BBB0-A43E0D055492", "cpe_name": [], "versionStartIncluding": "2.6.38", "versionEndExcluding": "4.19.312"}, {"vulnerable": true, "cpe23Uri": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "matchCriteriaId": "F45A0F3C-C16D-49C4-86D6-D021C3D4B834", "cpe_name": [], "versionStartIncluding": "4.20", "versionEndExcluding": "5.4.274"}, {"vulnerable": true, "cpe23Uri": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "matchCriteriaId": "9CD5894E-58E9-4B4A-B0F4-3E6BC134B8F5", "cpe_name": [], "versionStartIncluding": "5.5", "versionEndExcluding": "5.10.215"}, {"vulnerable": true, "cpe23Uri": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "matchCriteriaId": "577E212E-7E95-4A71-9B5C-F1D1A3AFFF46", "cpe_name": [], "versionStartIncluding": "5.11", "versionEndExcluding": "5.15.154"}, {"vulnerable": true, "cpe23Uri": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "matchCriteriaId": "834D9BD5-42A6-4D74-979E-4D6D93F630FD", "cpe_name": [], "versionStartIncluding": "5.16", "versionEndExcluding": "6.1.84"}, {"vulnerable": true, "cpe23Uri": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "matchCriteriaId": "8018C1D0-0A5F-48D0-BC72-A2B33FDDA693", "cpe_name": [], "versionStartIncluding": "6.2", "versionEndExcluding": "6.6.24"}, {"vulnerable": true, "cpe23Uri": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "matchCriteriaId": "6BE9771A-BAFD-4624-95F9-58D536540C53", "cpe_name": [], "versionStartIncluding": "6.7", "versionEndExcluding": "6.7.12"}, {"vulnerable": true, "cpe23Uri": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "matchCriteriaId": "4C59BBC3-6495-4A77-9C82-55EC7CDF5E02", "cpe_name": [], "versionStartIncluding": "6.8", "versionEndExcluding": "6.8.3"}]}, {"operator": "OR", "negate": false, "children": [], "cpe_match": [{"vulnerable": true, "cpe23Uri": "cpe:2.3:o:debian:debian_linux:10.0:*:*:*:*:*:*:*", "matchCriteriaId": "07B237A9-69A3-4A9C-9DA0-4E06BD37AE73", "cpe_name": []}]}]}}