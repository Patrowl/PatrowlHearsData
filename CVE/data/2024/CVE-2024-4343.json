{"cve": {"data_type": "CVE", "data_format": "MITRE", "data_version": "4.0", "CVE_data_meta": {"ID": "CVE-2024-4343", "ASSIGNER": "security@huntr.dev"}, "problemtype": {"problemtype_data": [{"description": []}]}, "references": {"reference_data": [{"url": "https://github.com/imartinez/privategpt/commit/86368c61760c9cee5d977131d23ad2a3e063cbe9", "name": "https://github.com/imartinez/privategpt/commit/86368c61760c9cee5d977131d23ad2a3e063cbe9", "refsource": "", "tags": []}, {"url": "https://huntr.com/bounties/1d1e8f06-ec45-4b17-ae24-b83a41304c15", "name": "https://huntr.com/bounties/1d1e8f06-ec45-4b17-ae24-b83a41304c15", "refsource": "", "tags": []}]}, "description": {"description_data": [{"lang": "en", "value": "A Python command injection vulnerability exists in the `SagemakerLLM` class's `complete()` method within `./private_gpt/components/llm/custom/sagemaker.py` of the imartinez/privategpt application, versions up to and including 0.3.0. The vulnerability arises due to the use of the `eval()` function to parse a string received from a remote AWS SageMaker LLM endpoint into a dictionary. This method of parsing is unsafe as it can execute arbitrary Python code contained within the response. An attacker can exploit this vulnerability by manipulating the response from the AWS SageMaker LLM endpoint to include malicious Python code, leading to potential execution of arbitrary commands on the system hosting the application. The issue is fixed in version 0.6.0."}]}}, "configurations": {"CVE_data_version": "4.0", "nodes": []}, "impact": {}, "publishedDate": "2024-11-14T18:15Z", "lastModifiedDate": "2024-11-18T21:35Z"}