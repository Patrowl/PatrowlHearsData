{"publishedDate": "2022-08-05T16:15Z", "lastModifiedDate": "2024-11-21T07:00Z", "cve": {"data_type": "CVE", "data_format": "MITRE", "data_version": "1.0", "CVE_data_meta": {"ID": "CVE-2022-2053", "ASSIGNER": "secalert@redhat.com"}, "description": {"description_data": [{"lang": "en", "value": "When a POST request comes through AJP and the request exceeds the max-post-size limit (maxEntitySize), Undertow's AjpServerRequestConduit implementation closes a connection without sending any response to the client/proxy. This behavior results in that a front-end proxy marking the backend worker (application server) as an error state and not forward requests to the worker for a while. In mod_cluster, this continues until the next STATUS request (10 seconds intervals) from the application server updates the server state. So, in the worst case, it can result in \"All workers are in error state\" and mod_cluster responds \"503 Service Unavailable\" for a while (up to 10 seconds). In mod_proxy_balancer, it does not forward requests to the worker until the \"retry\" timeout passes. However, luckily, mod_proxy_balancer has \"forcerecovery\" setting (On by default; this parameter can force the immediate recovery of all workers without considering the retry parameter of the workers if all workers of a balancer are in error state.). So, unlike mod_cluster, mod_proxy_balancer does not result in responding \"503 Service Unavailable\". An attacker could use this behavior to send a malicious request and trigger server errors, resulting in DoS (denial of service). This flaw was fixed in Undertow 2.2.19.Final, Undertow 2.3.0.Alpha2."}, {"lang": "es", "value": "Cuando una petici\u00f3n POST llega a trav\u00e9s de AJP y la petici\u00f3n excede el l\u00edmite de tama\u00f1o m\u00e1ximo de post (maxEntitySize), la implementaci\u00f3n AjpServerRequestConduit de Undertow cierra una conexi\u00f3n sin enviar ninguna respuesta al cliente/proxy. Este comportamiento resulta en que un proxy del front-end marque al trabajador del backend (servidor de aplicaciones) como un estado de error y no reenv\u00ede peticiones al trabajador durante un tiempo. En mod_cluster, esto contin\u00faa hasta que la siguiente petici\u00f3n de STATUS (en intervalos de 10 segundos) del servidor de aplicaciones actualiza el estado del servidor. As\u00ed que, en el peor de los casos, puede resultar en \"Todos los trabajadores est\u00e1n en estado de error\" y mod_cluster responde \"503 Service Unavailable\" durante un tiempo (hasta 10 segundos). En mod_proxy_balancer, no reenv\u00eda las peticiones al trabajador hasta que pasa el tiempo de espera de \"reintento\". Sin embargo, por suerte, mod_proxy_balancer presenta el par\u00e1metro \"forcerecovery\" (habilitado por defecto; este par\u00e1metro puede forzar la recuperaci\u00f3n inmediata de todos los trabajadores sin tener en cuenta el par\u00e1metro retry de los trabajadores si todos los trabajadores de un balanceador est\u00e1n en estado de error). As\u00ed, a diferencia de mod_cluster, mod_proxy_balancer no resulta en la respuesta \"503 Service Unavailable\". Un atacante podr\u00eda usar este comportamiento para enviar una petici\u00f3n maliciosa y desencadenar errores en el servidor, resultando en DoS (denegaci\u00f3n de servicio). Este fallo fue corregido en Undertow versi\u00f3n 2.2.19.Final, Undertow versi\u00f3n 2.3.0.Alpha2"}]}, "references": {"reference_data": [{"url": "https://bugzilla.redhat.com/show_bug.cgi?id=2095862&comment#0", "name": "", "refsource": "", "tags": ["Issue Tracking", "Vendor Advisory"]}, {"url": "https://issues.redhat.com/browse/UNDERTOW-2133", "name": "", "refsource": "", "tags": ["Vendor Advisory"]}, {"url": "https://bugzilla.redhat.com/show_bug.cgi?id=2095862&comment#0", "name": "", "refsource": "", "tags": ["Issue Tracking", "Vendor Advisory"]}, {"url": "https://issues.redhat.com/browse/UNDERTOW-2133", "name": "", "refsource": "", "tags": ["Vendor Advisory"]}]}, "problemtype": {"problemtype_data": [{"description": [{"lang": "en", "value": "CWE-400"}]}]}}, "impact": {"baseMetricV3": {"exploitabilityScore": 3.9, "impactScore": 3.6, "cvssV3": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:H", "baseScore": 7.5, "baseSeverity": "HIGH", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH"}}}, "configurations": {"CVE_data_version": "4.0", "nodes": [{"operator": "OR", "negate": false, "children": [], "cpe_match": [{"vulnerable": true, "cpe23Uri": "cpe:2.3:a:redhat:integration_camel_k:-:*:*:*:*:*:*:*", "matchCriteriaId": "B87C8AD3-8878-4546-86C2-BF411876648C", "cpe_name": []}, {"vulnerable": true, "cpe23Uri": "cpe:2.3:a:redhat:jboss_fuse:7.0.0:*:*:*:*:*:*:*", "matchCriteriaId": "B40CCE4F-EA2C-453D-BB76-6388767E5C6D", "cpe_name": []}, {"vulnerable": true, "cpe23Uri": "cpe:2.3:a:redhat:undertow:*:*:*:*:*:*:*:*", "matchCriteriaId": "581C0C48-DDC2-4781-9032-C5A0C2544C74", "cpe_name": [], "versionEndExcluding": "2.2.19"}, {"vulnerable": true, "cpe23Uri": "cpe:2.3:a:redhat:undertow:2.3.0:alpha1:*:*:*:*:*:*", "matchCriteriaId": "F6DD3DE4-9C5D-4768-9414-C63D1D172B7F", "cpe_name": []}]}]}}