{"id": "EUVD-2023-0118", "enisaUuid": "96ac99e7-5180-334b-96ff-ee645af1fe41", "description": "In LangChain through 0.0.131, the LLMMathChain chain allows prompt injection attacks that can execute arbitrary code via the Python exec method.", "datePublished": "2023-04-05T00:00:00", "dateUpdated": "2025-02-12T16:24:39", "baseScore": 9.8, "baseScoreVersion": "3.1", "baseScoreVector": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H", "references": ["https://twitter.com/rharang/status/1641899743608463365/photo/1", "https://github.com/hwchase17/langchain/pull/1119", "https://github.com/hwchase17/langchain/issues/814", "https://github.com/hwchase17/langchain/issues/1026", "https://nvd.nist.gov/vuln/detail/CVE-2023-29374", "https://github.com/langchain-ai/langchain", "https://github.com/pypa/advisory-database/tree/main/vulns/langchain/PYSEC-2023-18.yaml"], "aliases": ["CVE-2023-29374", "PYSEC-2023-18"], "assigner": "mitre", "epss": 2.74, "enisaIdProduct": [{"id": "701e35d0-631c-33ba-bfc1-6a38b725f420", "product": {"name": "n/a"}, "product_version": "n/a"}], "enisaIdVendor": [{"id": "1a3a3357-f198-3408-8357-ff8bf0466837", "vendor": {"name": "n/a"}}], "isExploited": false}