{"id": "EUVD-2025-198356", "enisaUuid": "b5d77d4f-6085-3626-a66c-50e00aedafa4", "description": "vLLM is an inference and serving engine for large language models (LLMs). From version 0.5.5 to before 0.11.1, the /v1/chat/completions and /tokenize endpoints allow a chat_template_kwargs request parameter that is used in the code before it is properly validated against the chat template. With the right chat_template_kwargs parameters, it is possible to block processing of the API server for long periods of time, delaying all other requests. This issue has been patched in version 0.11.1.", "datePublished": "2025-11-21T01:21:29", "dateUpdated": "2025-11-24T18:12:23", "baseScore": 6.5, "baseScoreVersion": "3.1", "baseScoreVector": "CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H", "references": ["https://github.com/vllm-project/vllm/security/advisories/GHSA-69j4-grxj-j64p", "https://github.com/vllm-project/vllm/pull/27205", "https://github.com/vllm-project/vllm/commit/3ada34f9cb4d1af763fdfa3b481862a93eb6bd2b", "https://github.com/vllm-project/vllm", "https://github.com/vllm-project/vllm/blob/2a6dc67eb520ddb9c4138d8b35ed6fe6226997fb/vllm/entrypoints/chat_utils.py#L1602-L1610", "https://github.com/vllm-project/vllm/blob/2a6dc67eb520ddb9c4138d8b35ed6fe6226997fb/vllm/entrypoints/openai/serving_engine.py#L809-L814", "https://nvd.nist.gov/vuln/detail/CVE-2025-62426"], "aliases": ["CVE-2025-62426", "GHSA-69j4-grxj-j64p"], "assigner": "GitHub_M", "epss": 0.06, "enisaIdProduct": [{"id": "5b536f83-4e2b-3a41-8c2b-1ad117feb946", "product": {"name": "vllm"}, "product_version": "0.5.5, < 0.11.1"}], "enisaIdVendor": [{"id": "6607fbe1-5192-3968-b5f5-f4abfdca97b6", "vendor": {"name": "vllm-project"}}], "isExploited": false}